#!/usr/bin/perl

# joint_entropy.pl v0.1.0
# Author: MH Seabolt
# Last updated: 8-21-2021

# SYNOPSIS
# Calculates the joint entropy between two random variables X and Y, given as two input matrices P (representing X) and Q (representing Y).
# Currently only handles joint entropy of TWO matrices (but could be run iteratively to produce joint entropy of multiple variables).
# Expects that both matrices are formatted in the same way.

# Note: This program internally calculates the joint probability distribution of the two random variables.  Do not supply a joint probability distribution as input to this program.
# If you already have a joint probability distribution, you can compute the joint entropy directly with my ShannonH program and the -m option set to cumulative ($ ShannonH -m cumulative)
##################################################################################
# The MIT License
#
# Copyright (c) 2021 Matthew H. Seabolt
#
# Permission is hereby granted, free of charge, 
# to any person obtaining a copy of this software and 
# associated documentation files (the "Software"), to 
# deal in the Software without restriction, including 
# without limitation the rights to use, copy, modify, 
# merge, publish, distribute, sublicense, and/or sell 
# copies of the Software, and to permit persons to whom 
# the Software is furnished to do so, 
# subject to the following conditions:
#
# The above copyright notice and this permission notice 
# shall be included in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, 
# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES 
# OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. 
# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR 
# ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, 
# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE 
# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
##################################################################################

use strict;
use warnings;
use Getopt::Long qw(GetOptions);

# GetOpts Variable declarations
my $input1 = "--";
my $input2;
my $output = "--";
my $cns = 1;
my $rns = 1;
my $sep;
my $outfmt;
my $method;

sub usage {
	my $usage = "joint_entropy.pl v0.1.0\n
	PURPOSE: Calculates the joint entropy between two random variables X and Y, given as two input matrices P (representing X) and Q (representing Y).
			Currently only handles joint entropy of TWO matrices (but could be run iteratively to produce joint entropy of multiple variables).
			Expects that both matrices are formatted in the same way.
			
			Note: This program internally calculates the joint probability distribution of the two random variables.  Do not supply
			a joint probability distribution as input to this program.
	\n
	USAGE:	joint_entropy.pl v0.1.0 -P data1.tab -Q data2.tab -o joint_entropy.tab
	-P		first input matrix P
	-Q 		second input matrix Q, formatted the same as the first matrix.
	-o 		output file name ( do not include extensions! )
	-c 		INT flag; matrix contains column headers? ( Default: ON )
	-r 		INT flag; matrix contains row names? ( Default: ON )
	-s 		STR; input data separator. ( Default: tab )
	-m 		STR; operation to use for joint probability, either 'and' for Pr(X and Y), or 'or' for Pr(X or Y).  ( Default: and )
\n";
	print $usage;
}

GetOptions(	'1|P=s' => \$input1, 
			'2|Q=s' => \$input2,
			'out|o=s' => \$output,
			'colnames|c=i' => \$cns,
			'rownames|r=i' => \$rns,
			'sep|s=s' => \$sep,
			'method|m=s' => \$method,
) or die usage();

# Parameter setups
$cns = ( $cns && $cns == 0 )? 0 : 1;
$rns = ( $rns && $rns == 0 )? 0 : 1;
$sep = ( $sep && $sep ne "\t" )? $sep : "\t";
$method = ( $method && $method =~ /(?im)^or/ )? "or" : "and";

##################################################################################
# Read the first input data file
my $fh = *STDIN;
my $succin = open(MATRIX1, "<", "$input1") if ( $input1 ne "--" && -e $input1 );
$fh = *MATRIX1 if ( $succin ); 

# Initialize a vector for rownames
my @rownames;

# Operate on the matrix line-by-line (here we are skipping column headers and rownames, if they are marked as being present
my @p_marginal;
my $p_sum = 0;
while ( <$fh> )		{
	chomp $_;
	if ( $cns == 1 && $. == 1 )		{
		next;
	}
	my @row = split("$sep", $_);
	if ( $rns == 1 )	{
		my $rowname = shift @row;
		push @rownames, $rowname;
	}
	my $row_sum = sum(@row);
	push @p_marginal, $row_sum;
	$p_sum += $row_sum;
}
close $fh if ( $succin );

##################################################################################
# Read the second matrix, which must come from a file and have the same formatting as the first matrix
open(MATRIX2, "<", "$input2" ) or die "joint_entropy::FATAL ERROR with 2nd input file $input2 --> $!\n";
	my @data2 = <MATRIX2>;
close MATRIX2;

# Operate on the matrix line-by-line
my @q_marginal;
my $q_sum = 0;
for ( my $i=0; $i < scalar @data2; $i++ )	{
	chomp $data2[$i];
	if ( $cns == 1 && $i == 0 )		{
		next;
	}
	my @row = split("$sep", $data2[$i]);
	if ( $rns == 1 )		{
		my $rowname = shift @row ;
	}
	my $row_sum = sum(@row);
	push @q_marginal, $row_sum;
	$q_sum += $row_sum;
}

# Sanity check
die "joint_entropy::FATAL ERROR --> Distributions do not have the same dimensions!\n" if ( scalar @p_marginal != scalar @q_marginal );

##################################################################################
# Calculate the joint distribution from the data

# Open the output filehandle
my $succout = open( OUT, ">", "$output" ) if $output ne "--";
my $fhout;
if ( $succout )		{	$fhout = *OUT;		}
else				{	$fhout = *STDOUT;	}
print $fhout "Nats\n" if ( $cns == 1 );

# Calculate the joint PROBABILITY distribution
my $joint_entropy = 0;
my $joint_total = $p_sum * $q_sum;
for ( my $i=0; $i < scalar @p_marginal; $i++ )		{
	my @joint_row;
	for ( my $j=0; $j < scalar @q_marginal; $j++ )	{
		( $method eq "and" )? push @joint_row, $p_marginal[$i] * $q_marginal[$j] : push @joint_row, $p_marginal[$i] + $q_marginal[$j];
	}
	$joint_entropy += Shannon(@joint_row);
}
if ( $rns == 1 )	{	print $fhout join("$sep", "H(x,y)", $joint_entropy),"\n";		}
else 				{ 	print $fhout "$joint_entropy\n";								}
close $fhout if ( $succout );
exit;


#################################### SUBROUTINES ##################################

# Calculates Shannon Entropy H'
sub Shannon		{
	my ( @sample ) = @_;
	
	# Error handling: return NA values if $N is zero
	my $N = sum( @sample );
	return ( 0 ) if ( $N == 0 );		
	
	# Calculate *p* for each species *i* <-- this can be thought of as relative abundance, but this is only one way to estimate that measure
	my @pis;
	for ( my $i=0; $i < scalar @sample; $i++ )	{
		$pis[$i] = ( $sample[$i] == 0 )? 0 : ($sample[$i]/$N) * log($sample[$i]/$N);
	}

	# Sum @pis and multiply by -1
	my $H = -( sum(@pis) );
	return $H;
}

sub sum		{
	my ( @list ) = @_;
	my $sum = 0;
	$sum += $_ foreach ( @list );
	return $sum;
}


